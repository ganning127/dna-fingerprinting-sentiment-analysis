{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "API Search and Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcDkN8dJpNdB"
      },
      "source": [
        "# Sentiment Analysis of the Public's Opinnion on DNA Fingerprinting using the Twitter API\n",
        "\n",
        "**Goals**\n",
        "- To analyze the public's opinnion on DNA fingerprinting using publically avaliable tweets. \n",
        "\n",
        "**Useful Links**\n",
        "- [Flowchart Overview](https://lucid.app/lucidchart/647ae773-5d75-4902-b463-f4ebaefc6a0e/edit?viewport_loc=-95%2C-163%2C2524%2C1568%2C0_0&invitationId=inv_c9da80f4-0c98-494f-98fa-f0a84c45c683)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vxi4lqjxSlq"
      },
      "source": [
        "## Install Dependencies\n",
        "Using `tweepy`, `gspread`, `colab-env`, `demoji`, and `emoji` python libraries.\n",
        "\n",
        "References\n",
        "- [tweepy](https://www.tweepy.org/): Used to query the Twitter API. \n",
        "- [gspread](https://docs.gspread.org/en/latest/): API for Google Spreadsheets.\n",
        "- [colab-env](https://pypi.org/project/colab-env/): Used for environment variables in Google Colab notebooks. \n",
        "- [demoji](https://pypi.org/project/demoji/): Removes and replaces emojis in text strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS1AflC5w1rA",
        "outputId": "f1493b2b-8329-4bd8-b7c0-ded4b1a9f452"
      },
      "source": [
        "!pip install tweepy\n",
        "!pip install --upgrade gspread\n",
        "!pip install colab-env --upgrade\n",
        "!pip install demoji\n",
        "!pip install textblob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Collecting gspread\n",
            "  Downloading gspread-4.0.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from gspread) (0.4.6)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from gspread) (1.35.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.1.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (1.24.3)\n",
            "Installing collected packages: gspread\n",
            "  Attempting uninstall: gspread\n",
            "    Found existing installation: gspread 3.0.1\n",
            "    Uninstalling gspread-3.0.1:\n",
            "      Successfully uninstalled gspread-3.0.1\n",
            "Successfully installed gspread-4.0.1\n",
            "Collecting colab-env\n",
            "  Downloading colab-env-0.2.0.tar.gz (4.7 kB)\n",
            "Collecting python-dotenv<1.0,>=0.10.0\n",
            "  Downloading python_dotenv-0.19.1-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: colab-env\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-env: filename=colab_env-0.2.0-py3-none-any.whl size=3836 sha256=bfd6c33a7f78a802d9e8196cee3ee78ed6326d656781975859d8a8b5b41c454d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ca/e8/3d25b6abb4ac719ecb9e837bb75f2a9b980430005fb12a9107\n",
            "Successfully built colab-env\n",
            "Installing collected packages: python-dotenv, colab-env\n",
            "Successfully installed colab-env-0.2.0 python-dotenv-0.19.1\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQQp7qvSFcK_"
      },
      "source": [
        "## Importing Dependencies\n",
        "*Also downloading information needed for `nltk` and `demoji`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82gUtQgFeeb",
        "outputId": "1c2ece54-0f71-4ba5-95e4-c78573776695"
      },
      "source": [
        "'''import dependencies'''\n",
        "import math \n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import colab_env\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import auth\n",
        "import os\n",
        "import demoji\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download([\n",
        "     \"names\",\n",
        "     \"stopwords\",\n",
        "     \"state_union\",\n",
        "     \"twitter_samples\",\n",
        "     \"movie_reviews\",\n",
        "     \"averaged_perceptron_tagger\",\n",
        "     \"vader_lexicon\",\n",
        "     \"punkt\",\n",
        "     \"brown\"])\n",
        "\n",
        "demoji.download_codes()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TyiL-CJxQcO"
      },
      "source": [
        "## Authorize Google Spreadsheets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsKoAIHZxCjJ"
      },
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp97-zuvEnsM"
      },
      "source": [
        "## Setting Environment Variables\n",
        "*Used to set environmnet variables for Twitter API keys.*\n",
        "\n",
        "[Google's Reference for Setting Environment Variables](https://colab.research.google.com/github/apolitical/colab-env/blob/master/colab_env_testbed.ipynb#scrollTo=PgQHBQfJDMMb)\n",
        "\n",
        "In order to get your API information, apply for a developer account [here](https://developer.twitter.com/en/apply-for-access)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJqxFLNjFrQW"
      },
      "source": [
        "colab_env.envvar_handler.add_env(\"API_KEY\", \"<YOUR API KEY>\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"API_SECRET_KEY\", \"<Your API SECRET KEY>\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"BEARER_TOKEN \", \"<YOUR BEARER TOKEN>\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"ACCESS_TOKEN\", \"<YOUR ACCESS TOKEN>\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"ACCESS_TOKEN_SECRET\", \"<YOUR ACCESS TOKEN SECRET>\", overwrite=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP-4aCeoxxL5"
      },
      "source": [
        "## Query the Twitter API\n",
        "\n",
        "\n",
        "### Overview\n",
        "1. Read keywords from the `keywords` sheet in the [[DATA] Public Sentiment on DNA Fingerprinting](https://docs.google.com/spreadsheets/d/1ern1BJY6qs84ZkLWYGsw02n2UL5tv_uyavh7VIKrDrg/edit?usp=sharing) workbook. \n",
        "2. Query the Twitter API for 100 English tweets for each keyword in the spreadsheet before a certain date. The date is changed every day to query for different tweets. \n",
        "3. Populate each raw tweet into their corrosponding date sheet. \n",
        "\n",
        "### Documentation\n",
        "\n",
        "#### Usage of `class Searcher`\n",
        "```python\n",
        "API_Searcher = Searcher(<workbook_name>, <sheet_name>, <cell_range>, <api_config>)\n",
        "API_Searcher.search_api(until=<YYYY-MM-DD>)\n",
        "API_Searcher.populate_tweets(<result_sheet_name>)\n",
        "```\n",
        "###### `__init__()` arguments\n",
        "- `workbook_name` - string. name of the Google Spreadsheet file that you are working on. Ex: `\"[DATA] Public Sentiment on DNA Fingerprinting\"`\n",
        "- `sheet_name` - string. name of the sheet within `workbook_name` that the keywords to search for are on. Ex: `\"keywords\"`\n",
        "- `cell_range` - string. cell range that the keywords are located. Blank cells are allowed and will automatically be skipped after the first blank cell. Ex: `\"C1:C1000\"`\n",
        "\n",
        "###### `API_Searcher.search_api()` arguments\n",
        "- `until` - string. Returns tweets created before the given date. Date should be formatted as YYYY-MM-DD. Keep in mind that the search index has a 7-day limit. In other words, no tweets will be found for a date older than one week. Ex: `\"2021-07-31\"`. [Tweepy Docs](https://docs.tweepy.org/en/stable/api.html#search-tweets)\n",
        "\n",
        "###### `API_Searcher.populate_tweets()` arguments\n",
        "- `sheet_name` - string. Name of the sheet in `workbook` to populate tweets found from the Twitter API into. \n",
        "\n",
        "### References\n",
        "- [Twitter API Docs](https://developer.twitter.com/en/docs)\n",
        "- [Tweepy API Docs](https://docs.tweepy.org/en/stable/api.html)\n",
        "- [GSpread Docs](https://docs.gspread.org/en/latest/user-guide.html#creating-a-worksheet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsUVDxzOxIRQ"
      },
      "source": [
        "'''Read keywords from the keywords sheet in the [DATA] Public Sentiment on DNA Fingerprinting workbook.'''\n",
        "\n",
        "class Searcher():\n",
        "  def __init__(self, workbook, sheet, range, api_config):\n",
        "    self.workbook = workbook\n",
        "    self.sheet = sheet\n",
        "    self.range = range\n",
        "    self._api_config = api_config\n",
        "    self.api = self._create_api()\n",
        "    self.keywords = self._read_keywords()\n",
        "    self.word_to_tweets = {}\n",
        "    \n",
        "  def _create_api(self): \n",
        "    auth = tweepy.OAuthHandler(self._api_config['apiKey'], self._api_config['apiSecretKey'])\n",
        "    auth.set_access_token(self._api_config['accessToken'], self._api_config['accessTokenSecret'])\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "    return api\n",
        "      \n",
        "  def _read_keywords(self):\n",
        "    # function reads a cell range in google sheets and returns a python list\n",
        "    workbook = gc.open(self.workbook) # workbooks\n",
        "    # save sheets\n",
        "    worksheet = workbook.worksheet(self.sheet)\n",
        "    # read data from sheets\n",
        "    cell_list = worksheet.range(self.range)\n",
        "\n",
        "    keywords = self._read_cells(cell_list)\n",
        "    return keywords \n",
        "\n",
        "  def _read_cells(self, cell_list):\n",
        "    # reads the cells in the cell_list\n",
        "    lst = []\n",
        "    for cell in cell_list:\n",
        "      if cell.value:\n",
        "        lst.append(cell.value) # cells with values are added to list\n",
        "      else:\n",
        "        break # empty cells are not read\n",
        "    return lst\n",
        "\n",
        "  def _convert_tweets(self, word, tweets):\n",
        "    self.word_to_tweets[word] = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "      if 'retweeted_status' in tweet._json:\n",
        "        text = tweet._json['retweeted_status']['full_text']\n",
        "      else:\n",
        "        text = tweet.full_text\n",
        "\n",
        "      if text in self.word_to_tweets[word]:\n",
        "        # self.word_to_tweets[word].append(\"\")\n",
        "        pass\n",
        "      else:\n",
        "        self.word_to_tweets[word].append(text)\n",
        "\n",
        "\n",
        "  def search_api(self, until):\n",
        "    for word in self.keywords:\n",
        "      tweets = self.api.search(word, count=100, tweet_mode='extended', lang='en', until=until) # list of tweets (text acessible with tweet.text)\n",
        "      self._convert_tweets(word, tweets) # add tweets to dictionary mapping keyword to list of tweets\n",
        "      print(\"done: \" + word + \"...\")\n",
        "    \n",
        "  def populate_tweets(self, sheet_name):\n",
        "    workbook = gc.open(self.workbook) # entire workbook\n",
        "    try:\n",
        "      worksheet_to_delete = workbook.worksheet(sheet_name)\n",
        "      workbook.del_worksheet(worksheet_to_delete)\n",
        "    except:\n",
        "      # worksheet does not exist, create a new one\n",
        "      pass\n",
        "    worksheet = workbook.add_worksheet(title=sheet_name, rows=\"1000\", cols=str(len(self.keywords)))\n",
        "    dataframe = pd.DataFrame.from_dict(self.word_to_tweets, orient='index')\n",
        "    dataframe = dataframe.transpose()\n",
        "    worksheet.update([dataframe.columns.values.tolist()] + dataframe.values.tolist())\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH0sT1j42_XE"
      },
      "source": [
        "api_config = {\n",
        "    \"apiKey\": os.getenv(\"API_KEY\"),\n",
        "    \"apiSecretKey\": os.getenv(\"API_SECRET_KEY\"),\n",
        "    \"bearerToken\": os.getenv(\"BEARER_TOKEN\"),\n",
        "    \"accessToken\": os.getenv(\"ACCESS_TOKEN\"),\n",
        "    \"accessTokenSecret\": os.getenv(\"ACCESS_TOKEN_SECRET\"),\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDmRIs8a3gEm",
        "outputId": "82d964de-7b18-479c-9438-6cdaf30ed26e"
      },
      "source": [
        "dates = ['2021-11-03'] # change this line to query for different dates\n",
        "dates_to_sheet_name = {\n",
        "    date : date[5:] + \"_raw\" for date in dates\n",
        "}\n",
        "\n",
        "for date in dates:\n",
        "  API_Searcher = Searcher('[DATA] Public Sentiment on DNA Fingerprinting', 'keywords', 'B4:B1000', api_config)\n",
        "  API_Searcher.search_api(until=date)\n",
        "  API_Searcher.populate_tweets(dates_to_sheet_name[date])\n",
        "  print(\"--------------stats for\", date, \"--------------\")\n",
        "  for key in API_Searcher.word_to_tweets:\n",
        "    print(len(API_Searcher.word_to_tweets[key]), \"tweets for\", key)\n",
        "  print(\"\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done: dna fingerprinting...\n",
            "done: dna fingerprint...\n",
            "done: genetic fingerprinting...\n",
            "done: genetic fingerprint...\n",
            "done: dna identification...\n",
            "done: dna profiling...\n",
            "done: dna profile...\n",
            "done: dna typing...\n",
            "done: genetic profile...\n",
            "done: genetic profiling...\n",
            "--------------stats for 2021-11-03 --------------\n",
            "37 tweets for dna fingerprinting\n",
            "49 tweets for dna fingerprint\n",
            "4 tweets for genetic fingerprinting\n",
            "7 tweets for genetic fingerprint\n",
            "67 tweets for dna identification\n",
            "28 tweets for dna profiling\n",
            "45 tweets for dna profile\n",
            "13 tweets for dna typing\n",
            "46 tweets for genetic profile\n",
            "12 tweets for genetic profiling\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKLvndN-6Y6"
      },
      "source": [
        "## Data Merging & Cleaning\n",
        "As a result of the Querying the Twitter API, there are many different sheets created that each represent a timespan in which tweets were pulled back from. In order to perform sentiment analysis on the entire dataset, all tweets need to first be merged into a comon spredsheet and be cleaned. \n",
        "\n",
        "### Overview of Data Merging\n",
        "1. Go through each sheet that ends in `_raw`, and collect all the tweets in each column into a dictionary. Duplicate tweets are ignored (since the different queries through time spans may have overlap). \n",
        "2. Populate the combined data into a new sheet called `merged_tweets`.\n",
        "\n",
        "### Overview of Data Cleaning\n",
        "After data merging, each tweet in the `merged_tweets` spreadsheet is cleaned. Data cleaning removes the unimportant filler words, as well as converts other parts of typical English text into information that sentiment analyzers can understand. \n",
        "1. All image URLs are replaced with `_IMG`\n",
        "2. All regular URLS are replaced with `_URL`\n",
        "3. All emojis are replaced with their textual descriptions (`🙂` → `Slightly Smiling Face`)\n",
        "4. All repeated letters are replaced with just 2 letters of the same repeated character (`heeeeeeeello` → `heello`)\n",
        "\n",
        "Example Tweet:\n",
        "- Before Cleaning: \n",
        "```We 💚love💚 these photos of some very impressive students learning gel electrophoresis and DNA profiling ... in first year! 🤯 Thank you for sharing the photos @GoreyEtss. We're looking forward to seeing what these scientists do next! #BiotechExperience @ABEProgOffice https://t.co/idow3wAkSd```\n",
        "- After Cleaning:\n",
        "```: green heart : love : green heart : photos impressive students learning gel electrophoresis DNA profiling .. first year ! : exploding head : Thank sharing photos @ GoreyEtss . 're looking forward seeing scientists next ! #BiotechExperience @ ABEProgOffice _IMAGE```\n",
        "\n",
        "### Documentation\n",
        "#### Usage of `class Cleaner`\n",
        "```python\n",
        "CleanerObj = Cleaner()\n",
        "CleanerObj.clean_text(<text>)\n",
        "```\n",
        "###### `clean_text()` arguments\n",
        "- `text` - string. the text that will be put through the cleaning process mentioned above.\n",
        "\n",
        "#### Usage of `class Tweet_Cleaner`\n",
        "```python\n",
        "test = Tweet_Cleaner(workbook=<workbook_name>, match_phrase=<match_phrase>, result_sheet=<merge_result_sheet>, clean_sheet=<clean_tweet_sheet>)\n",
        "test.merge_tweets()\n",
        "test.clean_tweets()\n",
        "```\n",
        "\n",
        "- `workbook_name` - string. name of the workbook containing tweets.\n",
        "- `match_phrase` - string. last ending few letters that will be matched against to figure out which sheets contain raw tweets.\n",
        "- `merge_result_sheet` - string. name of the spreadsheet to populate all of the tweets from each time span of raw tweets to. \n",
        "- `clean_sheet` - string. name of the spreadsheet to output all of the cleaned tweets to. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbiZIQry_FbE"
      },
      "source": [
        "class Cleaner:\n",
        "  def replace_emoji_and_dup(self, text):\n",
        "    '''\n",
        "    function takes in string `text` parameter and removes all duplicate characters in a row that are greater than 2 \n",
        "    -replaces the emojis with their descriptions\n",
        "    '''\n",
        "    tracker = {}\n",
        "    final = []\n",
        "\n",
        "    # check if the past two characters were that same char\n",
        "\n",
        "    for i in range(len(text)):\n",
        "      if i < 2:\n",
        "        final.append(text[i])\n",
        "        continue\n",
        "      if (text[i-1] == text[i]) and (text[i-2] == text[i]):\n",
        "        pass\n",
        "      else:\n",
        "        final.append(text[i])\n",
        "\n",
        "    final = \"\".join(final)\n",
        "    return demoji.replace_with_desc(final)\n",
        "\n",
        "  # Example \n",
        "\n",
        "  def replace_image(self, text):\n",
        "    '''\n",
        "    function takes a `string` parameter text and replaces all image URLS with `_IMAGE` \n",
        "    '''\n",
        "    return re.sub(r\"https://t.co/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\w+\", \"_IMAGE\", text)\n",
        "    \n",
        "\n",
        "  def replace_url(self, text):\n",
        "    '''\n",
        "    function takes a `string` parameter text and replaces all URLS with `_URL` \n",
        "    '''\n",
        "    return re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", ' _URL', text)\n",
        "\n",
        "\n",
        "  def remove_stopwords(self, text):\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    tweet_words = nltk.word_tokenize(text)\n",
        "    words = [w for w in tweet_words if w.lower() not in stopwords]\n",
        "    clean_tweet = \" \".join(words)\n",
        "    return clean_tweet\n",
        "\n",
        "  def clean_text(self, text):\n",
        "    '''\n",
        "    MASTER FUNCTION\n",
        "    '''\n",
        "    # remove emojis and duplicate chars\n",
        "    cleaned = self.replace_emoji_and_dup(text)\n",
        "\n",
        "   \n",
        "\n",
        "    # replace image links with _IMAGE\n",
        "    cleaned = self.replace_image(cleaned)\n",
        "\n",
        "    # remove stopwords\n",
        "    cleaned = self.remove_stopwords(cleaned)\n",
        "\n",
        "    # replace links with _URL\n",
        "    cleaned = self.replace_url(cleaned)\n",
        "\n",
        "    return cleaned"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZfdNT1kAGHD"
      },
      "source": [
        "class Tweet_Cleaner():\n",
        "    def __init__(self, workbook, match_phrase, result_sheet, clean_sheet):\n",
        "      self.workbook = gc.open(workbook)\n",
        "      self.match_phrase = match_phrase\n",
        "      self.result_sheet = result_sheet\n",
        "      self.raw_tweet_sheets = self._get_raw_tweet_sheets() # list\n",
        "      self.word_to_tweets = {}\n",
        "      self.clean_sheet = clean_sheet\n",
        "\n",
        "    def _get_raw_tweet_sheets(self):\n",
        "      # adds all worksheets with tweets that need to be cleaned into a list\n",
        "      valid_sheets = []\n",
        "      worksheet_list = self.workbook.worksheets()\n",
        "      for sheet in worksheet_list:\n",
        "        # if sheet.title[-4:] == self.match_phrase:\n",
        "        if sheet.title[-len(self.match_phrase):] == self.match_phrase:\n",
        "          valid_sheets.append(sheet)\n",
        "      return valid_sheets\n",
        "\n",
        "    def merge_tweets(self):\n",
        "      for sheet in self.raw_tweet_sheets:\n",
        "        list_of_dicts = sheet.get_all_records()\n",
        "        self._add_to_data(list_of_dicts)\n",
        "      self._populate_tweets(self.result_sheet, self.word_to_tweets)\n",
        "\n",
        "    def _add_to_data(self, list_of_dicts):\n",
        "      # self.word_to_tweets should contain key -> all the tweets from raw sheets\n",
        "      for dictionary in list_of_dicts:\n",
        "        for key in dictionary:\n",
        "          # key is dna_fingerprinting, as an example\n",
        "          if dictionary[key] == \"\":\n",
        "            continue\n",
        "          if key in self.word_to_tweets:\n",
        "            # append item to list, if not blank\n",
        "              if dictionary[key] in self.word_to_tweets[key]:\n",
        "                pass\n",
        "              else:\n",
        "                self.word_to_tweets[key].append(dictionary[key])\n",
        "          else:\n",
        "              self.word_to_tweets[key] = [dictionary[key]]\n",
        "    def _populate_tweets(self, sheet_name, data):\n",
        "      try:\n",
        "        worksheet_to_delete = self.workbook.worksheet(sheet_name)\n",
        "        self.workbook.del_worksheet(worksheet_to_delete)\n",
        "      except:\n",
        "        # worksheet does not exist, create a new one\n",
        "        pass\n",
        "      worksheet = self.workbook.add_worksheet(title=sheet_name, rows=\"1000\", cols=str(len(data.keys())))\n",
        "      dataframe = pd.DataFrame.from_dict(data, orient='index')\n",
        "      dataframe = dataframe.transpose()\n",
        "      worksheet.update([dataframe.columns.values.tolist()] + dataframe.values.tolist())\n",
        "\n",
        "    def clean_tweets(self):\n",
        "      CleanerObj = Cleaner()\n",
        "      for key in self.word_to_tweets:\n",
        "        for i in range(len(self.word_to_tweets[key])):\n",
        "          cleaned = CleanerObj.clean_text(self.word_to_tweets[key][i])\n",
        "          self.word_to_tweets[key][i] = cleaned\n",
        "      self._populate_tweets(self.clean_sheet, self.word_to_tweets)\n",
        "      \n",
        "          \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDAuqTijAt5Q"
      },
      "source": [
        "test = Tweet_Cleaner(workbook=\"[DATA] Public Sentiment on DNA Fingerprinting\", match_phrase=\"_raw\", result_sheet=\"merged_tweets\", clean_sheet=\"cleaned_tweets\")\n",
        "test.merge_tweets()\n",
        "test.clean_tweets()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS6YqCPmanJo"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "After populating all cleaned tweets into a spreadsheet, it is possible to perform sentiment analysis on the data. \n",
        "\n",
        "### Overview of Sentiment Analysis\n",
        "1. Go through each tweet under each keyword, and get the sentiment score from that tweet using the [TextBlob](https://textblob.readthedocs.io/en/dev/) Python library. \n",
        "2. Store each sentiment and subjectivity score into a list\n",
        "3. Find the average of each list in order to determine the average sentiment for each keyword\n",
        "4. Output the average sentiment for each keyword to the console.\n",
        "5. Grab the [noun phrases](https://textblob.readthedocs.io/en/dev/quickstart.html#noun-phrase-extraction) and populate them into another sheet.\n",
        "\n",
        "### Sentiment, Subjectivity, and Noun Phrases\n",
        "- **Sentiment**: A float between -1.0 - 1.0 in which -1.0 represents the most negative langauge while 1.0 represents the most positive langauge. \n",
        "- **Subjectivity**: A float between 0.0 - 1.0 in which a higher value represents more subjective text. \n",
        "- **Noun Phrases**: List of nouns within a tweet\n",
        "\n",
        "### Documentation\n",
        "Usage of `class Tweet_Analyzer`\n",
        "\n",
        "```py\n",
        "Tweet_Machine = Tweet_Analyzer(\"<workbook_name>\", \"<sentiment_sheet_name>\")\n",
        "Tweet_Machine.analyze_sentiment()\n",
        "Tweet_Machine.get_most_used_keywords(result_sheet=\"<phrase_result_sheet_name>\")\n",
        "```\n",
        "\n",
        "- `workbook_name` - string. name of the workbook containing tweets.\n",
        "- `sentiment_sheet_name` - string. name of the worksheet containing the tweets that you want to perform sentiment analysis on.\n",
        "- `phrase_result_sheet_name` - string. name of the worksheet to populate all noun phrases onto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lziybMrWpH13"
      },
      "source": [
        "class Tweet_Analyzer():\n",
        "  def __init__(self, workbook, sheet_name):\n",
        "    self.workbook = gc.open(workbook)\n",
        "    self.worksheet = self.workbook.worksheet(sheet_name)\n",
        "    self.word_to_tweets = {}\n",
        "    self.sentiment_results = {} # keyword to each sentiment score\n",
        "    self.subjectivity_results = {} # higher subj = more likely to be an opinnion; lower subj = morel likely to be factual info\n",
        "    self.noun_lists = {}\n",
        "\n",
        "  def analyze_sentiment(self):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    self._get_all_tweets() # populates self.word_to_tweets\n",
        "    # go through each key and each list within that key to conduct sentiment analysis\n",
        "    for keyword in self.word_to_tweets:\n",
        "      # self.word_to_tweets[key] is a list of all tweets\n",
        "      for tweet in self.word_to_tweets[keyword]:\n",
        "        tweet_analyze = TextBlob(tweet)\n",
        "        score = sia.polarity_scores(str(tweet))\n",
        "        self.sentiment_results[keyword].append(score['compound'])\n",
        "        self.subjectivity_results[keyword].append(tweet_analyze.sentiment.subjectivity)\n",
        "\n",
        "    print(\"results for sentiment results:\")\n",
        "    \n",
        "    overall_sentiment = self._get_final_results(self.sentiment_results)\n",
        "    df_1 = pd.DataFrame.from_dict(overall_sentiment, orient=\"index\")\n",
        "    print(df_1)\n",
        "    \n",
        "    print()\n",
        "\n",
        "    print(\"results for subjectivity results:\")\n",
        "    overall_subjectivity = self._get_final_results(self.subjectivity_results)\n",
        "    df_2 = pd.DataFrame.from_dict(overall_subjectivity, orient=\"index\")\n",
        "    print(df_2)\n",
        "    \n",
        "  def _get_final_results(self, dictionary):\n",
        "    results = {}\n",
        "    for keyword in dictionary:\n",
        "      total = sum(dictionary[keyword])\n",
        "      count = len(dictionary[keyword])\n",
        "      results[keyword] = round(total/count, 2)\n",
        "    return results\n",
        "\n",
        "  def _get_all_tweets(self):\n",
        "    list_of_dicts = self.worksheet.get_all_records()\n",
        "    self._add_to_data(list_of_dicts)\n",
        "\n",
        "  def _add_to_data(self, list_of_dicts):\n",
        "    # self.word_to_tweets should contain key -> all the tweets from cleaned sheet\n",
        "    for dictionary in list_of_dicts:\n",
        "      for key in dictionary:\n",
        "        # key is dna_fingerprinting, as an example\n",
        "        if dictionary[key] == \"\":\n",
        "          continue\n",
        "        if key in self.word_to_tweets:\n",
        "          # append item to list, if not blank\n",
        "            if dictionary[key] in self.word_to_tweets[key]:\n",
        "              pass\n",
        "            else:\n",
        "              self.word_to_tweets[key].append(dictionary[key])\n",
        "        else:\n",
        "            self.word_to_tweets[key] = [dictionary[key]]\n",
        "            self.sentiment_results[key] = []\n",
        "            self.subjectivity_results[key] = []\n",
        "\n",
        "  def get_most_used_keywords(self, result_sheet):\n",
        "    self._get_all_tweets()\n",
        "    for keyword in self.word_to_tweets:\n",
        "      # self.word_to_tweets[key] is a list of all tweets\n",
        "      for tweet in self.word_to_tweets[keyword]:\n",
        "        tweet_analyze = TextBlob(tweet)\n",
        "        tweet_noun_phrases = list(tweet_analyze.noun_phrases)\n",
        "        tweet_noun_phrases = self._clean_phrases(tweet_noun_phrases)\n",
        "        self._add_to_noun_lists(keyword, tweet_noun_phrases)\n",
        "    # print(self.noun_lists) # noun lists maps keyword to a list of all noun phrases from all tweets in that category\n",
        "    self._add_to_sheet(result_sheet)\n",
        "\n",
        "  def _add_to_sheet(self, result_sheet):\n",
        "    try:\n",
        "      worksheet_to_delete = self.workbook.worksheet(result_sheet)\n",
        "      self.workbook.del_worksheet(worksheet_to_delete)\n",
        "    except:\n",
        "      # worksheet does not exist, create a new one\n",
        "      pass\n",
        "    \n",
        "    keys = self.noun_lists.keys()\n",
        "    worksheet = self.workbook.add_worksheet(title=result_sheet, rows=\"1000\", cols=str(len(keys)))\n",
        "    dataframe = pd.DataFrame.from_dict(self.noun_lists, orient='index')\n",
        "    dataframe = dataframe.transpose()\n",
        "    worksheet.update([dataframe.columns.values.tolist()] + dataframe.values.tolist())\n",
        "\n",
        "\n",
        "  def _add_to_noun_lists(self, keyword, lst):\n",
        "    if keyword in self.noun_lists:\n",
        "      self.noun_lists[keyword].extend(lst)\n",
        "    else:\n",
        "      self.noun_lists[keyword] = lst\n",
        "\n",
        "  def _clean_phrases(self, lst):\n",
        "    new_phrases = []\n",
        "    for item in lst:\n",
        "      if item[0] != \"@\":\n",
        "        new_phrases.append(item)\n",
        "    return new_phrases\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6QEKRf9qB0_",
        "outputId": "986ca6c5-41ef-40c9-c0dd-0e26159105fb"
      },
      "source": [
        "Tweet_Machine = Tweet_Analyzer(\"[DATA] Public Sentiment on DNA Fingerprinting\", \"cleaned_tweets\")\n",
        "Tweet_Machine.analyze_sentiment()\n",
        "Tweet_Machine.get_most_used_keywords(result_sheet=\"phrases\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results for sentiment results:\n",
            "                           0\n",
            "dna fingerprint         0.06\n",
            "genetic fingerprint     0.13\n",
            "dna identification      0.04\n",
            "dna profiling           0.02\n",
            "dna fingerprinting      0.10\n",
            "dna profile             0.09\n",
            "genetic fingerprinting -0.07\n",
            "dna typing              0.05\n",
            "genetic profile         0.10\n",
            "genetic profiling       0.08\n",
            "\n",
            "results for subjectivity results:\n",
            "                           0\n",
            "dna fingerprint         0.39\n",
            "genetic fingerprint     0.40\n",
            "dna identification      0.40\n",
            "dna profiling           0.38\n",
            "dna fingerprinting      0.35\n",
            "dna profile             0.41\n",
            "genetic fingerprinting  0.37\n",
            "dna typing              0.43\n",
            "genetic profile         0.40\n",
            "genetic profiling       0.45\n"
          ]
        }
      ]
    }
  ]
}